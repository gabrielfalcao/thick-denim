<html><head><title>An Overview of Lexing and Parsing</title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" >
<link rel="stylesheet" type="text/css" title="pod_stylesheet" href="/assets/css/local/pod.css">

</head>
<body class='pod'>
<!--
  generated by Local::Pod::Simple::HTML v1.03,
  using Pod::Simple::PullParser v3.22,
  under Perl v5.014002 at Tue Nov 13 00:00:51 2012 GMT.

 If you want to change this HTML document, you probably shouldn't do that
   by changing it directly.  Instead, see about changing the calling options
   to Local::Pod::Simple::HTML, and/or subclassing Local::Pod::Simple::HTML,
   then reconverting this document from the Pod source.
   When in doubt, email the author of Local::Pod::Simple::HTML for advice.
   See 'perldoc Local::Pod::Simple::HTML' for more info.

-->

<!-- start doc -->
<a name='___top' class='dummyTopAnchor' ></a>
<h1><div class="toc_title">An Overview of Lexing and Parsing</div></h1>
<h3><div class="toc_toc">Table of contents</div></h3>
<table align="center" summary = "Table of Contents">
<tr><td align="center"><a href="#An_Overview_of_Lexing_and_Parsing">An Overview of Lexing and Parsing</a></td></tr>
<tr><td align="center"><a href="#A_History_Lesson_-_In_Absentia">A History Lesson - In Absentia</a></td></tr>
<tr><td align="center"><a href="#But_Why_Study_Lexing_and_Parsing%3F">But Why Study Lexing and Parsing?</a></td></tr>
<tr><td align="center"><a href="#Good_Solutions_and_Home-grown_Solutions">Good Solutions and Home-grown Solutions</a></td></tr>
<tr><td align="center"><a href="#The_Lexer%27s_Job_Description">The Lexer&#39;s Job Description</a></td></tr>
<tr><td align="center"><a href="#The_Parser%27s_Job_Description">The Parser&#39;s Job Description</a></td></tr>
<tr><td align="center"><a href="#Grammars_and_Sub-grammars">Grammars and Sub-grammars</a></td></tr>
<tr><td align="center"><a href="#Sub-grammar_%23_1">Sub-grammar # 1</a></td></tr>
<tr><td align="center"><a href="#Sub-grammar_%23_2">Sub-grammar # 2</a></td></tr>
<tr><td align="center"><a href="#My_Golden_Rule_of_Lexing_and_Parsing">My Golden Rule of Lexing and Parsing</a></td></tr>
<tr><td align="center"><a href="#In_Case_You_Think_This_is_Going_to_be_Complex">In Case You Think This is Going to be Complex</a></td></tr>
<tr><td align="center"><a href="#Coding_the_Lexer">Coding the Lexer</a></td></tr>
<tr><td align="center"><a href="#Back_to_the_Finite_Automaton">Back to the Finite Automaton</a></td></tr>
<tr><td align="center"><a href="#States">States</a></td></tr>
<tr><td align="center"><a href="#Sample_Lexer_Code">Sample Lexer Code</a></td></tr>
<tr><td align="center"><a href="#Coding_the_Lexer_-_Revisited">Coding the Lexer - Revisited</a></td></tr>
<tr><td align="center"><a href="#Coding_the_Parser">Coding the Parser</a></td></tr>
<tr><td align="center"><a href="#Sample_Parser_Code">Sample Parser Code</a></td></tr>
<tr><td align="center"><a href="#Connecting_the_Parser_back_to_the_Lexer">Connecting the Parser back to the Lexer</a></td></tr>
<tr><td align="center"><a href="#Something_Fascinating_about_Rule_Descriptors">Something Fascinating about Rule Descriptors</a></td></tr>
<tr><td align="center"><a href="#Chains_and_Trees">Chains and Trees</a></td></tr>
<tr><td align="center"><a href="#Less_Coding_-_More_Design">Less Coding - More Design</a></td></tr>
<tr><td align="center"><a href="#My_Rules-of-Thumb_for_Writing_Lexers%2FParsers">My Rules-of-Thumb for Writing Lexers/Parsers</a></td></tr>
<tr><td align="center"><a href="#Eschew_Premature_Optimisation">Eschew Premature Optimisation</a></td></tr>
<tr><td align="center"><a href="#Divide_and_Conquer">Divide and Conquer</a></td></tr>
<tr><td align="center"><a href="#Don%27t_Reinvent_the_Wheel">Don&#39;t Reinvent the Wheel</a></td></tr>
<tr><td align="center"><a href="#Be_Patient_with_the_STT">Be Patient with the STT</a></td></tr>
<tr><td align="center"><a href="#Be_Patient_with_the_Grammar">Be Patient with the Grammar</a></td></tr>
<tr><td align="center"><a href="#Wrapping_Up_and_Winding_Down">Wrapping Up and Winding Down</a></td></tr>
<tr><td align="center"><a href="#Author">Author</a></td></tr>
<tr><td align="center"><a href="#Licence">Licence</a></td></tr>
</table>
<h1><a class='u' href='#___top' title='click to go to top of document'
name="An_Overview_of_Lexing_and_Parsing"
>An Overview of Lexing and Parsing</a></h1>

<p>For a more formal discussion than this article of what exactly lexing and parsing are,
start with Wikipedia&#39;s definitions: <a href="http://en.wikipedia.org/wiki/Lexing" class="podlinkurl"
>Lexing</a> and <a href="http://en.wikipedia.org/wiki/Parsing" class="podlinkurl"
>Parsing</a>.</p>

<p>Also,
the word parsing sometimes includes lexing and sometimes doesn&#39;t.
This can cause confusion,
but I&#39;ll try to keep them clear.
Such situations arise with other words,
and our minds usually resolve the specific meaning intended by analysing the context in which the word is used.
So,
keep your mind in mind.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="A_History_Lesson_-_In_Absentia"
>A History Lesson - In Absentia</a></h1>

<p>At this point,
an article such as this would normally provide a summary of historical developments in this field,
as in &#39;how we got to here&#39;.
I won&#39;t do that,
especially as I first encountered parsing many years ago,
when the only tools (lex,
bison,
yacc) were so complex to operate I took the pledge to abstain.
Nevertheless,
it&#39;s good to know such tools are still available,
so here are a few references:</p>

<p><a href="http://directory.fsf.org/wiki/Flex" class="podlinkurl"
>Flex</a> is a successor to <a href="http://en.wikipedia.org/wiki/Lex_programming_tool" class="podlinkurl"
>lex</a>,
and <a href="http://www.gnu.org/software/bison/" class="podlinkurl"
>Bison</a> is a successor to <a href="http://en.wikipedia.org/wiki/Yacc" class="podlinkurl"
>yacc</a>.
This article explains why I (still) don&#39;t use any of these.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="But_Why_Study_Lexing_and_Parsing?"
>But Why Study Lexing and Parsing?</a></h1>

<p>There are many situations where the only path to a solution requires a lexer and a parser.</p>

<p>The lex phase and the parse phase can be combined into a single process,
but I advocate always keeping them separate,
and I aim below to demonstrate why this is the best policy.</p>

<p>Also,
for beginners,
note that the phases very conveniently run in alphabetical order: We lex and then we parse.</p>

<p>So,
let&#39;s consider some typical situations where lexing and parsing are the tools needed:</p>

<dl>
<dt><a name="1:_Running_a_program"
>1: Running a program</a></dt>

<dd>
<p>This is trivial to understand,
but not to implement.
In order to run a program we need to set up a range of pre-conditions:</p>

<dl>
<dt><a name="o_Define_the_language,_perhaps_called_Perl"
>o Define the language,
perhaps called Perl</a></dt>

<dd>
<dt><a name="o_Write_a_compiler_(combined_lexer_and_parser)_for_that_language&#39;s_grammar"
>o Write a compiler (combined lexer and parser) for that language&#39;s grammar</a></dt>

<dd>
<dt><a name="o_Write_a_program_in_that_language"
>o Write a program in that language</a></dt>

<dd>
<dt><a name="o_Lex_and_parse_the_source_code"
>o <i>Lex and parse</i> the source code</a></dt>

<dd>
<p>After all,
it must be syntactically correct before we run it.
If not,
we display syntax errors.</p>

<p>The real point of this step is to determine the programmer&#39;s <i>intention</i>,
i.e.
what is the reason for writing the code?
Not to run it actually,
but to get the output.
And how do we do that?</p>

<dt><a name="o_Run_the_code"
>o Run the code</a></dt>

<dd>
<p>Then we can gaze at the output which,
hopefully,
is correct.
Or,
perhaps,
we are confronted by logic errors.</p>
</dd>
</dl>

<dt><a name="2:_Rendering_a_web_page_of_HTML_+_Content"
>2: Rendering a web page of HTML + Content</a></dt>

<dd>
<p>The steps are identical to the above,
with HTML replacing Perl,
althought I can&#39;t bring myself to call writing HTML writing a program.</p>

<p>This time,
we&#39;re asking: What is the web page designer&#39;s <i>intention</i>,
i.e.
how exactly do they want the page to be rendered?</p>

<p>Of course,
syntax checking is far looser than with a programming language,
but must still be undertaken.
For instance,
here&#39;s an example of clearly-corrupt HTML which can be parsed by <a href="http://jeffreykegler.github.com/Marpa-web-site/" class="podlinkurl"
>Marpa</a>:</p>

<pre>        &#60;title&#62;Short&#60;/title&#62;&#60;p&#62;Text&#60;/head&#62;&#60;head&#62;</pre>

<p>See <a href="http://metacpan.org/module/Marpa::R2::HTML" class="podlinkpod"
>Marpa::R2::HTML</a> for details. The original version of Marpa has been superceded by Marpa::R2. Now I use <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a> in all my work (which happens to not involve HTML).</p>

<dt><a name="3:_Rendering_an_image,_perhaps_in_SVG"
>3: Rendering an image, perhaps in SVG</a></dt>

<dd>
<p>Consider this file, written in the <a href="http://www.graphviz.org/content/dot-language" class="podlinkurl"
>DOT</a> language, as used by the <a href="http://www.graphviz.org/" class="podlinkurl"
>Graphviz</a> graph visualizer (<i>teamwork.dot</i>):</p>

<pre>        digraph Perl
        {
        graph [ rankdir=&#34;LR&#34; ]
        node  [ fontsize=&#34;12pt&#34; shape=&#34;rectangle&#34; style=&#34;filled, solid&#34; ]
        edge  [ color=&#34;grey&#34; ]
        &#34;Teamwork&#34; [ fillcolor=&#34;yellow&#34; ]
        &#34;Victory&#34;  [ fillcolor=&#34;red&#34; ]
        &#34;Teamwork&#34; -&#62; &#34;Victory&#34; [ label=&#34;is the key to&#34; ]
        }</pre>

<p>Here we have a &#39;program&#39; and we wish to give effect to the author&#39;s <i>intention</i> by rendering this as an image:</p>

<img src="http://savage.net.au/Ron/html/graphviz2.marpa/teamwork.svg" />


<p>What&#39;s required to do that? As above, <i>lex, parse</i>, render. Using Graphviz&#39;s <i>dot</i> command to carry out these 3 tasks, we would run:</p>

<pre>        shell&#62; dot -Tsvg teamwork.dot &#62; teamwork.svg</pre>

<p>Note: Files used in these examples can be downloaded from <a href="http://savage.net.au/Ron/html/graphviz2.marpa/teamwork.tgz" class="podlinkurl"
>http://savage.net.au/Ron/html/graphviz2.marpa/teamwork.tgz</a>.</p>

<p>Now, the above link to the DOT language points to a definition of DOT&#39;s syntax, written in a somewhat casual version of BNF: <a href="http://en.wikipedia.org/wiki/Backus%E2%80%93Naur_Form" class="podlinkurl"
>Backus-Naur Form</a>. This is significant, since it&#39;s usually straight-forward to translate a BNF description of a language into code within a lexer and parser.</p>

<dt><a name="4:_Rendering_that_same_image,_using_a_different_language_in_the_input_file"
>4: Rendering that same image, using a different language in the input file</a></dt>

<dd>
<p>Let&#39;s say we feel the Graphviz language is too complex, and hence we write a wrapper around it, so end users can code in a simplified version of that language. This has been done, with the original effort available in the now-obsolete Perl module <a href="http://metacpan.org/module/Graph::Easy" class="podlinkpod"
>Graph::Easy</a>. Tels, the author, devised his own very clever <a href="http://en.wikipedia.org/wiki/Little_languages" class="podlinkurl"
>little language</a>, which he called Graph::Easy. The manual for that is on-line <a href="http://bloodgate.com/perl/graph/manual/" class="podlinkurl"
>here</a>.</p>

<p>When I took over maintenance of <a href="http://metacpan.org/module/Graph::Easy" class="podlinkpod"
>Graph::Easy</a>, I found the code too complex to read, let alone work on, so I wrote another implementation of the lexer and parser, released as <a href="http://metacpan.org/module/Graph::Easy::Marpa" class="podlinkpod"
>Graph::Easy::Marpa</a>. I&#39;ll have much more to say about that in the next article in this 2-part series, so for now we&#39;ll just examine the above graph re-cast in Graph::Easy (<i>teamwork.easy</i>):</p>

<pre>        graph {rankdir: LR}
        node {fontsize: 12pt; shape: rectangle; style: filled, solid}
        edge {color: grey}
        [Teamwork]{fillcolor: yellow}
        -&#62; {label: is the key to}
        [Victory]{fillcolor: red}</pre>

<p>Simpler for sure, but how does <a href="http://metacpan.org/module/Graph::Easy::Marpa" class="podlinkpod"
>Graph::Easy::Marpa</a> work? As always: <i>lex, parse</i>, render. More samples of <a href="http://metacpan.org/module/Graph::Easy::Marpa" class="podlinkpod"
>Graph::Easy::Marpa</a>&#39;s work are <a href="http://savage.net.au/Perl-modules/html/graph.easy.marpa/index.html" class="podlinkurl"
>here</a>.</p>
</dd>
</dl>

<p>It should be clear by now that lexing and parsing are in fact widespread, although they often operate out of sight, with just their rendered output visible to the average programmer and web surfer.</p>

<p>What all such problems have in common is complex but well-structured source text formats, with a bit of hand-waving over the tacky details available to authors of documents in HTML. And, in each case, it is the responsibility of the programmer writing the lexer and parser to honour the intention of the original text&#39;s author.</p>

<p>And we can only do that by recognizing each token in the input as embodying some meaning (e.g. a word such as &#39;print&#39; <i>means</i> output something of the author&#39;s choosing), and bringing that meaning to fruition (make the output visible on a device).</p>

<p>With all that I can safely claim that it&#39;s the ubiquity and success of lexing and parsing which justify their recognition as vital constituents in the world of software engineering. And with that we&#39;re done answering the question posed above: Why study them?</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Good_Solutions_and_Home-grown_Solutions"
>Good Solutions and Home-grown Solutions</a></h1>

<p>But there&#39;s another - significant - reason to discuss lexing and parsing. And that&#39;s to train programmers, without expertise in such matters, to resist the understandable urge to opt for using tools they are already familiar with, with regexps being the &#39;obvious&#39; choice.</p>

<p>Sure, regexps suit many simple cases, and the old standbys of flex and bison are always available, but now there&#39;s a new kid on the block: <a href="http://www.jeffreykegler.com/marpa" class="podlinkurl"
>Marpa</a>.</p>

<p>Marpa is heavily based on theoretical work done over many decades, and comes in various forms:</p>

<dl>
<dt><a name="o_libmarpa"
>o libmarpa</a></dt>

<dd>
<p>Hand-crafted in C.</p>

<dt><a name="o_Marpa::XS"
>o Marpa::XS</a></dt>

<dd>
<p>The Perl and C-based interface to the <i>previous</i> version of libmarpa.</p>

<dt><a name="o_Marpa::R2"
>o Marpa::R2</a></dt>

<dd>
<p>The Perl and C-based interface to the most recent version of libmarpa. This is the version I use.</p>

<dt><a name="o_Marpa::R2::Advanced::Thin"
>o Marpa::R2::Advanced::Thin</a></dt>

<dd>
<p>The newest and thinnest interface to libmarpa, which documents how to make Marpa accessible to non-Perl languages.</p>
</dd>
</dl>

<p>The problem, of course, is whether or not any of these are a good, or even excellent, choice.</p>

<p>Marpa&#39;s advantages are huge, and can be summarized as:</p>

<dl>
<dt><a name="o_Is_well_tested"
>o Is well tested</a></dt>

<dd>
<p>This alone is of great significance.</p>

<dt><a name="o_Has_a_Perl_interface"
>o Has a Perl interface</a></dt>

<dd>
<p>This means I can specify the task in Perl, and let Marpa handle the details.</p>

<dt><a name="o_Has_its_own_Google_Group"
>o Has its own Google Group</a></dt>

<dd>
<p>See <a href="http://groups.google.com/group/marpa-parser?hl=en" class="podlinkurl"
>http://groups.google.com/group/marpa-parser?hl=en</a></p>

<dt><a name="o_Is_already_used_by_various_modules_on_CPAN_(this_search_keyed_to_Marpa)"
>o Is already used by various modules on <a href="https://metacpan.org/search?q=Marpa" class="podlinkurl"
>CPAN</a> (this search keyed to Marpa)</a></dt>

<dd>
<p>Hence, Open Source says you can see exactly how other people use it.</p>

<dt><a name="o_Has_a_very_simple_syntax"
>o Has a very simple syntax</a></dt>

<dd>
<p>Once you get used to it, of course! And if you&#39;re having trouble, just post on the Google Group.</p>

<p>Actually, if you&#39;ve ever worked with flex and bison, you&#39;ll be astonished at how simple it is to drive Marpa.</p>

<dt><a name="o_Is_very_fast_(libmarpa_is_written_in_C)"
>o Is very fast (libmarpa is written in C)</a></dt>

<dd>
<p>This is a bit surprising, since new technology usually needs some time to surpass established technology while delivering the all-important stability.</p>

<dt><a name="o_Is_being_improved_all_the_time"
>o Is being improved all the time</a></dt>

<dd>
<p>For instance, recently the author eliminated the dependency on Glib, to improve portability.</p>

<p>What&#39;s important is that this work is on-going, and we can expect a series of incremental improvements for some time to come.</p>
</dd>
</dl>

<p>So, some awareness of the choice of tools is important long before coding begins.</p>

<p>BTW: I use Marpa in <a href="http://metacpan.org/module/Graph::Easy::Marpa" class="podlinkpod"
>Graph::Easy::Marpa</a> and <a href="http://metacpan.org/module/GraphViz2::Marpa" class="podlinkpod"
>GraphViz2::Marpa</a>.</p>

<p>However, this is not an article on Marpa (but the next one is), so let&#39;s return to discussing lexing and parsing.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="The_Lexer&#39;s_Job_Description"
>The Lexer&#39;s Job Description</a></h1>

<p>As mentioned, the stages, conveniently, run in English alphabetical order, so we lex and then we parse.</p>

<p>Here, I&#39;m using lexing to mean the comparatively simple process of tokenising a stream of text, which means chopping that input stream into discrete tokens, and identifying the type of each token. The output is a new stream, this time of stand-alone tokens. And I say &#39;comparatively&#39; because I see parsing as complex <i>compared to</i> lexing.</p>

<p>And no, lexing does not do anything more than identify tokens. Therefore questions about the meanings of those tokens, or their acceptable order, are matters for the parser.</p>

<p>So, the lexer will say: I have found another token, and have identified it as being of some type T. Hence, for each recognized token, 2 items will be output:</p>

<dl>
<dt><a name="o_The_type_of_the_token"
>o The type of the token</a></dt>

<dd>
<dt><a name="o_The_value_of_the_token"
>o The value of the token</a></dt>
</dl>

<p>Since the process happens repeatedly, the output will be an array of token elements, with each element needing at least these 2 components: type and value.</p>

<p>In practice, I prefer to represent these elements as a hashref, like this:</p>

<pre>        {
                count =&#62; $integer, # 1 .. N.
                name  =&#62; &#39;&#39;,       # Unused.
                type  =&#62; $string,  # The type of the token.
                value =&#62; $value,   # The value from the input stream.
        }</pre>

<p>with the array being managed by an object of type <a href="http://metacpan.org/module/Set::Array" class="podlinkpod"
>Set::Array</a>, which I did not write but which I do now maintain. The advantage of Set::Array over <a href="http://metacpan.org/module/Set::Tiny" class="podlinkpod"
>Set::Tiny</a> is that the former preserves the ordering of the elements. See also <a href="http://savage.net.au/Perl-modules/html/setops.report.html" class="podlinkurl"
>this report</a> comparing a range of set-handling modules.</p>

<p>The &#39;count&#39; field, apparently redundant, is sometimes employed in the clean-up phase of the lexer, which may need to combine tokens unnecessarily split by the regexp-based approach. Also, it is available to the parser if needed, so I always include it in the hashref.</p>

<p>The &#39;name&#39; field really is unused, but gives people who fork or sub-class my code a place to work with their own requirements, without worrying that their edits will affect the fundamental code.</p>

<p>BTW, if you have an application where the output is best stored in a tree, the Perl module <a href="http://metacpan.org/module/Tree::DAG_Node" class="podlinkpod"
>Tree::DAG_Node</a> is superb (and which I also did not write but now maintain).</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="The_Parser&#39;s_Job_Description"
>The Parser&#39;s Job Description</a></h1>

<p>The parser then, concerns itself with the context in which each token appears, which is a way of saying it cares about whether or not the sequence and combination of tokens actually detected fits the expected grammar.</p>

<p>Ideally, the grammar is provided in BNF Form. This makes it easy to translate into the form acceptable to Marpa. If it&#39;s not in that form, you&#39;re work is (probably) going to be harder, simply because someone else has <i>not</i> done the hard work formalizing the grammar.</p>

<p>And now it&#39;s time to expand on &#39;grammars&#39;.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Grammars_and_Sub-grammars"
>Grammars and Sub-grammars</a></h1>

<p>An example grammar was mentioned above: <a href="http://www.graphviz.org/content/dot-language" class="podlinkurl"
>DOT</a>. But, how are we to understand a block of text written in BNF? Well, training is of course required when taking on such a task, and to that I&#39;d add what I&#39;ve gleaned from this work, as follows.</p>

<p>To us beginners eventually comes the realization that grammars, no matter how formally defined or otherwise, contain within them 2 sub-grammars:</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Sub-grammar_#_1"
>Sub-grammar # 1</a></h2>

<p>One sub-grammar specifies what a token looks like, meaning what range of forms it can assume in the input stream. If an incomprehensible candidate it detected, the lexer can generate an error, or it can activate a strategy called - by Jeffrey Kegler, the author of <a href="http://www.jeffreykegler.com/marpa" class="podlinkurl"
>Marpa</a> - <a href="http://blogs.perl.org/users/jeffrey_kegler/2011/11/marpa-and-the-ruby-slippers.html" class="podlinkurl"
>Ruby Slippers</a> (which has no relation to the Ruby programming language).</p>

<p>Put simply, the Ruby Slippers strategy fiddles the current token, or perhaps an even larger section of the input stream, in a way that satisfies the grammar, and restarts processing at the new current token. Marpa is arguably unique in being able to do this.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Sub-grammar_#_2"
>Sub-grammar # 2</a></h2>

<p>The other sub-grammar specifies how these tokens are allowed to combine, meaning if they don&#39;t conform to the grammar, the code generates a syntax error of some sort.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="My_Golden_Rule_of_Lexing_and_Parsing"
>My Golden Rule of Lexing and Parsing</a></h2>

<p>It is: <i>We will encode the first sub-grammar into the lexer and the second into the parser.</i></p>

<p>This says that if we know what tokens look like, we can tokenize the input stream, i.e. split it into separate tokens using a lexer. Then we give those tokens to the parser for (more) syntax checking, and for interpretation of what the user presumably intended with that specific input stream (combination of tokens).</p>

<p>And that gives us a clear plan-of-attack when confronted by a new project.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="In_Case_You_Think_This_is_Going_to_be_Complex"
>In Case You Think This is Going to be Complex</a></h1>

<p>Truely, it just <i>sounds</i> complicated, because I&#39;ll be introducing various presumably-new concepts, but don&#39;t dispair. It&#39;s not really that difficult.</p>

<p>We have to, somehow and somewhere, manage the complexity of the question &#39;Is this a valid document?&#39; for a given grammar.</p>

<p>Recognizing a token with a regex is easy. Keeping track of the context in which we see that token, and the context in which our grammar allows that token, are hard.</p>

<p>Yes, the complexity of setting up and managing a formal grammar and the DFA (see below) seems like a lot of work, but it&#39;s a specified and well understood mechanism we don&#39;t have to reinvent something every time.</p>

<p>By limiting the code we have to write two things: a set of rules for how to construct tokens within a grammar, and a set of rules for what happens when we construct a valid combination of tokens, we can focus on the important part of our application - determining what a document which conforms to the grammar means (the author&#39;s <i>intention</i>) - and less on the mechanics of verifying that a document matches the grammar.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Coding_the_Lexer"
>Coding the Lexer</a></h1>

<p>Here&#39;s how it works: The lexer&#39;s job is to recognise tokens, and our sub-grammar #1 specifies what they look like. So our lexer will have to examine the input stream, possibly one character at a time, to see if the &#39;current&#39; input, appended to the immediately preceding input, fits the definition of a token.</p>

<p>Now, a programmer can write a lexer anyway they want of course. The way I do it is with regexps combined with a DFA (<a href="http://en.wikipedia.org/wiki/Deterministic_finite_automaton" class="podlinkurl"
>Discrete Finite Automaton</a>) module. This blog entry - <a href="http://blogs.perl.org/users/andrew_rodland/2012/01/more-marpa-madness.html" class="podlinkurl"
>More Marpa Madness</a> - discusses using Marpa in the lexer (i.e. as well as in the parser, which is where I use it).</p>

<p>And what is a DFA? Abusing any reasonable definition, let me describe them thusly: The &#39;Finite&#39; part means the input stream only contains a limited number of different tokens, which simplifies the code, and the &#39;Automata&#39; in our case is the software machine we are writing, i.e. the program. For more, see that Wikipedia article on DFAs. BTW: DFAs are often known by their nick, STT (State Transition Table).</p>

<p>And now, what&#39;s this <a href="http://en.wikipedia.org/wiki/State_transition_table" class="podlinkurl"
>State Transition Table (STT)</a>? It is precisely the set of regexps which recognise tokens, together with instructions about what to do when a specific type of token is recognised.</p>

<p>But how do we make this all work? <a href="https://metacpan.org/" class="podlinkurl"
>MetaCPAN</a> is your friend! In particular, we&#39;ll use <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a> to drive the process. For candidate alternatives I assembled a list of Perl modules with relevance in the area, while cleaning up the docs for Set::FA. See <a href="https://metacpan.org/module/Set::FA#See-Also" class="podlinkurl"
>https://metacpan.org/module/Set::FA#See-Also</a>. I did not write <a href="http://metacpan.org/module/Set::FA" class="podlinkpod"
>Set::FA</a>, nor <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a>, but I now maintain them.</p>

<p>Be assured, such a transformation of BNF (or whatever our grammar&#39;s source is) into a DFA gives us:</p>

<dl>
<dt><a name="o_Insight_into_the_problem"
>o Insight into the problem</a></dt>

<dd>
<p>To cast BNF into regexps means we must understand exactly what the grammar definition is saying.</p>

<dt><a name="o_Clarity_of_formulation"
>o Clarity of formulation</a></dt>

<dd>
<p>We end up with a spreadsheet which simply and clearly encodes our understanding of tokens.</p>

<p>Spreadsheet? Yes, I store the derived regexps, along with other information, in a spreadsheet, as explained below. Techniques for incorporating this spreadsheet into the source code are discussed shortly.</p>
</dd>
</dl>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Back_to_the_Finite_Automaton"
>Back to the Finite Automaton</a></h1>

<p>In practice, we simply read and re-read, many times, the definition of our BNF (here the <a href="http://www.graphviz.org/content/dot-language" class="podlinkurl"
>DOT</a> language), and build up the corresponding set of regexps to handle each &#39;case&#39;. This is labourious work, no doubt about it.</p>

<p>For example, by using a regexp like /[a-zA-Z_][a-zA-Z_0-9]*/, we can get Perl&#39;s regexp engine to intelligently gobble up characters as long as they fit the definition. Here, the regexp is just saying: Start with a letter, upper- or lower-case, or an underline, followed by 0 or more letters, digits or underlines. Look familiar? It&#39;s very close to the Perl definition of \w, but disallows leading digits. Actually, <a href="http://www.graphviz.org/content/dot-language" class="podlinkurl"
>DOT</a> disallows them (in certain circumstances), so we have to, but of course it (DOT) does allow pure numbers in certain circumstances.</p>

<p>And what do we do with all these hand-crafted regexps? We use them as <i>data</i> to feed into the DFA, along with the input stream. The output of the DFA is basically a flag saying Yes/No, the input stream matches/doesn&#39;t match the token definitions specified by the given regexps. Along the way, the DFA calls one of our callback functions each time a token is successfully recognized, so we can stockpile them. Then, at the end of the run, we can output them as a stream of tokens, each with its identifying &#39;type&#39;, as per &#39;The Lexer&#39;s Job Description&#39; above.</p>

<p>A note about callbacks: Sometimes it&#39;s easier to design a regexp to capture more than seems appropriate, and to use code in the callback to chop up what&#39;s been captured, outputting several token elements as a consequence.</p>

<p>Since developing the STT is such an iterative process, you&#39;ll want various test files, and some scripts with short names to run the tests. Short names because you&#39;re going to be running these scripts an unbelievable number of times....</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="States"
>States</a></h1>

<p>So, what are states and why do we care about them?</p>

<p>Well, at any instant our STT (automation, software machine) is in precisely 1 (one) state. Perhaps it has not yet received even 1 token (it&#39;s in the &#39;start&#39; state), or perhaps it has just finished processing the previous one. Whatever, the code maintains information so as to know exactly what state it is in, and this leads to knowing exactly what set of tokens is now acceptable. I.e. any one of this set will be a legal token in the current state.</p>

<p>This is telling us then that we have to associate each regexp with a specific state, and visa versa, and it&#39;s implicitly telling us that we stay in the current state as long as each new input character matches a regexp belonging to the current state, and that we jump (transition) to a new state when that character does not match.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Sample_Lexer_Code"
>Sample Lexer Code</a></h1>

<p>Consider this simplistic code from the synopsis of <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a>:</p>

<pre>        my($dfa) = Set::FA::Element -&#62; new
        (
                accepting   =&#62; [&#39;baz&#39;],
                start       =&#62; &#39;foo&#39;,
                transitions =&#62;
                [
                        [&#39;foo&#39;, &#39;b&#39;, &#39;bar&#39;],
                        [&#39;foo&#39;, &#39;.&#39;, &#39;foo&#39;],
                        [&#39;bar&#39;, &#39;a&#39;, &#39;foo&#39;],
                        [&#39;bar&#39;, &#39;b&#39;, &#39;bar&#39;],
                        [&#39;bar&#39;, &#39;c&#39;, &#39;baz&#39;],
                        [&#39;baz&#39;, &#39;.&#39;, &#39;baz&#39;],
                ],
        );</pre>

<p>In the <i>transitions</i> parameter the first line says: &#39;foo&#39; is a state&#39;s name, and &#39;b&#39; is a regexp saying we jump to state &#39;bar&#39; if the next input char is &#39;b&#39;. And so on.</p>

<p>This in turn tells us that to use <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a> we have to prepare a transitions parameter matching this format. Hence the need for states and regexps.</p>

<p>And this is code I&#39;ve used, taken directly from <a href="http://metacpan.org/module/GraphViz2::Marpa::Lexer::DFA" class="podlinkpod"
>GraphViz2::Marpa::Lexer::DFA</a>:</p>

<pre>        Set::FA::Element -&#62; new
        (
                accepting   =&#62; \@accept,
                actions     =&#62; \%actions,
                die_on_loop =&#62; 1,
                logger      =&#62; $self -&#62; logger,
                start       =&#62; $self -&#62; start,
                transitions =&#62; \@transitions,
                verbose     =&#62; $self -&#62; verbose,
        );</pre>

<p>Let&#39;s discuss these parameters.</p>

<dl>
<dt><a name="o_accepting"
>o accepting</a></dt>

<dd>
<p>This is an arrayref of state names, meaning that, after processing the entire input stream, if we end up in one of these states, then we have &#39;accepted&#39; that input stream. That just means that every input token matched an appropriate regexp, where &#39;appropriate&#39; means every char matched the regexp belonging to the current state, whatever the state was at the instant that char was input.</p>

<dt><a name="o_actions"
>o actions</a></dt>

<dd>
<p>This is a hashref of function names, so we can call a function, optionally, upon entering or leaving any state. That&#39;s how we stockpile recognized tokens.</p>

<p>Further, since we write these functions, and we attach each to a particular combination of state and regexp, we encode into the function the knowledge of what &#39;type&#39; of token has just been matched when the DFA calls a function. And that&#39;s how our stockpile will end up with (token, type) pairs to output at the end of the run.</p>

<dt><a name="o_die_on_loop"
>o die_on_loop</a></dt>

<dd>
<p>This flag, if true, tells the DFA to stop if none of the regexps belonging to the current state match the current input char, i.e. stop rather than loop for ever.</p>

<p>You might wonder what stopping automatically is not the default, or even mandatory. Well, it&#39;s because you might was to try some recovery algorithm in such a situation, before dying.</p>

<dt><a name="o_logger"
>o logger</a></dt>

<dd>
<p>This is an (optional) logger objct.</p>

<dt><a name="o_start"
>o start</a></dt>

<dd>
<p>This is the name of the state in which the STT starts, so the code knows which regexp(s) to try upon inputting the very first char.</p>

<dt><a name="o_transitions"
>o transitions</a></dt>

<dd>
<p>This is a potentially large arrayref, listing separately for all states all the regexps which are to be invoked, one at a time, in the current state, while trying to match the current input char.</p>

<dt><a name="o_verbose"
>o verbose</a></dt>

<dd>
<p>Specifies how much to report if the logger object is not defined.</p>
</dd>
</dl>

<p>So, the next problem is how to prepare the grammar in such a way as to fit into this parameter list, and hence addressing that must come next.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Coding_the_Lexer_-_Revisited"
>Coding the Lexer - Revisited</a></h1>

<p>The coder thus needs to develop regexps etc which can be fed directly into the chosen DFA, here <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a>, or which can be transformed somehow into a format acceptable to that module. But so far I haven&#39;t actually said how I do that. It&#39;s time to be explicit....</p>

<p>I use a spreadsheet with 9 columns:</p>

<dl>
<dt><a name="o_Start"
>o Start</a></dt>

<dd>
<p>This just contains 1 word, &#39;Yes&#39;, against the name of the state which is the start state.</p>

<dt><a name="o_Accept"
>o Accept</a></dt>

<dd>
<p>This contains the word &#39;Yes&#39; against the name of any state which will be an accepting state (see above).</p>

<dt><a name="o_State"
>o State</a></dt>

<dd>
<p>This is the name of the state.</p>

<dt><a name="o_Event"
>o Event</a></dt>

<dd>
<p>This is a regexp, which is read to mean: The event fires (is triggered) if the current input char matches the regexp in this column.</p>

<p>Now, since the regexp belongs to a given state, we know the DFA will only process regexps associated with the current state, of which there will be 1 or at most a few.</p>

<p>When there are multiple regexps per state, I leave all other columns empty.</p>

<dt><a name="o_Next"
>o Next</a></dt>

<dd>
<p>The name of the &#39;next&#39; state, i.e. the name of the state to which the STT will jump if the current char matches the regexp given on the same line of the spreadsheet (in the current state of course).</p>

<dt><a name="o_Entry"
>o Entry</a></dt>

<dd>
<p>The optional name of the function the DFA is to call upon (just before) entry to the (new) state.</p>

<dt><a name="o_Exit"
>o Exit</a></dt>

<dd>
<p>The optional name of the function the DFA is to call upon exiting from the current state.</p>

<dt><a name="o_Regexp"
>o Regexp</a></dt>

<dd>
<p>This is a working column, in which I put formulas so that I can refer to them in various places in the &#39;Event&#39; column. It is not passed to the DFA in the &#39;transitions&#39; parameter.</p>

<dt><a name="o_Interpretation"
>o Interpretation</a></dt>

<dd>
<p>Comments to self.</p>
</dd>
</dl>

<p>The STT for <a href="http://metacpan.org/module/GraphViz2::Marpa" class="podlinkpod"
>GraphViz2::Marpa</a> is <a href="http://savage.net.au/Perl-modules/html/graphviz2.marpa/stt.html" class="podlinkurl"
>on-line here</a>.</p>

<p>Now, this structure has various advantages:</p>

<dl>
<dt><a name="o_Legibility"
>o Legibility</a></dt>

<dd>
<p>It is very easy to read, and to work with. Don&#39;t forget, to start with you&#39;ll be basically switching back and forth between the grammar definition document (hopefully in BNF) and this spreadsheet. This is a way of saying there&#39;s no coding done at this stage.</p>

<dt><a name="o_Exportability"
>o Exportability</a></dt>

<dd>
<p>Since I have not yet addressed the question of how the code will read the spreadsheet, I offer these techniques:</p>

<dl>
<dt><a name="o_Read_the_spreadsheet_directly"
>o Read the spreadsheet directly</a></dt>

<dd>
<p>There is no problem with this method, except the complexity of the code (in the external module which does the reading of course), and the slowness of loading and running this code.</p>

<p>Actually, I should mention that since I use <a href="http://www.libreoffice.org/" class="podlinkurl"
>LibreOffice</a> I can either force end-users to install <a href="http://metacpan.org/module/OpenOffice::OODoc" class="podlinkpod"
>OpenOffice::OODoc</a>, or export the spreadsheet as an Excel file, in order to avail themselves of this option. I have chosen to not support reading the *.ods file directly in the modules (<a href="http://metacpan.org/module/Graph::Easy::Marpa" class="podlinkpod"
>Graph::Easy::Marpa</a> and <a href="http://metacpan.org/module/GraphViz2::Marpa" class="podlinkpod"
>GraphViz2::Marpa</a>) I ship.</p>

<dt><a name="o_Export_the_spreadsheet_to_a_CSV_file_first"
>o Export the spreadsheet to a CSV file first</a></dt>

<dd>
<p>This way, we can read a CSV file into the DFA fairly quickly, without loading the module which reads spreadsheets.</p>

<p>Be careful here with LibreOffice, since it forces you to use Unicode for the spreadsheet, but exports e.g. double-quotes as the 3 byte sequence 0xe2, 0x80, 0x9c, which when used in a regexp will never match a &#39;real&#39; double-quote in your input stream. Sigh. Do No Evil. If only. So, when exporting, <i>always</i> choose the ASCII option.</p>

<dt><a name="o_Incorporate_the_spreadsheet_directly_into_our_code_(my_favourite)"
>o Incorporate the spreadsheet directly into our code (my favourite)</a></dt>

<dd>
<p>We do this in 2 stages: Export to a CSV file, and just use an editor to append that file to the end of the source code of our module, after the __DATA__ token.</p>

<p>Such in-line data can be accessed effortlessly by the very neat and very fast module <a href="http://metacpan.org/module/Data::Section::Simple" class="podlinkpod"
>Data::Section::Simple</a>. Clearly, since our module has been loaded already, because it&#39;s precisely what&#39;s being executed, there is essentially no overhead whatsoever in reading data from within it. Don&#39;t you just love Perl! And MetaCPAN of course. And a community which contributes such wonderous code.</p>

<p>An advantage of this alternative is that it lets end-users edit the shipped *.csv or *.ods files, after which they can use a command line option on scripts to read their file, over-riding the built-in STT.</p>
</dd>
</dl>
</dd>
</dl>

<p>After all this, it&#39;s just a matter of code which read and validates the structure of the STT&#39;s data, and then reformats it into what <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a> demands.</p>

<p>So, enough about the lexer, but we can say that by now the first sub-grammar has been incorporated into the design and code of the lexer, and that the second sub-grammar must now be encoded into the parser, for that&#39;s how the parser performs syntax checking.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Coding_the_Parser"
>Coding the Parser</a></h1>

<p>How we do this depends intimately on which pre-existing module, if any, we choose to use to aid the development of the parser. Since I choose Marpa (currently <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a>), I am orienting this article to that module. However, only in the next article will I deal in depth with Marpa.</p>

<p>Whichever module is chosen, you can think of the process like this: Our input stream is a set of pre-defined tokens (probably but not necessarily output from the lexer), and we must now specify all possible legal combinations of those tokens, i.e. the syntax of the language. This really means <i>the remainder</i> of the syntax, since by now we&#39;ve lost interest in the definition of a (legal) token. I.e. at this point we are assuming all incoming tokens are legal, which is a way of saying we will not try to parse and run a program containing token-based syntax errors, although it may contain logic errors (even if written in Perl :-).</p>

<p>Then, a combination of tokens which does not match any of the given legal combinations can be immediately rejected as a syntax error. And keep in mind that the friendliest compilers find as many syntax errors as possible per parse.</p>

<p>And since this check takes place on a token-by-token basis, we (ought to) know precisely which token triggered the error, which means we can emit a nice error message, identifying the culprit and its context.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Sample_Parser_Code"
>Sample Parser Code</a></h1>

<p>Here&#39;s a sample of a Marpa::R2 grammar (adapted from its synopsis):</p>

<pre>        my($grammar) = Marpa::R2::Grammar -&#62; new
        ({
                actions =&#62; &#39;My_Actions&#39;,
                start   =&#62; &#39;Expression&#39;,
                rules   =&#62;
                [
                        { lhs =&#62; &#39;Expression&#39;, rhs =&#62; [qw/Term/] },
                        { lhs =&#62; &#39;Term&#39;,       rhs =&#62; [qw/Factor/] },
                        { lhs =&#62; &#39;Factor&#39;,     rhs =&#62; [qw/Number/] },
                        { lhs =&#62; &#39;Term&#39;,       rhs =&#62; [qw/Term Add Term/],
                                action =&#62; &#39;do_add&#39;
                        },
                        { lhs =&#62; &#39;Factor&#39;,     rhs =&#62; [qw/Factor Multiply Factor/],
                                action =&#62; &#39;do_multiply&#39;
                        },
                ],
                default_action =&#62; &#39;do_something&#39;,
        });</pre>

<p>We need to understand these parameters before being able to write something like this for our chosen grammar.</p>

<p>Now, despite the differences between this and the calls to Set::FA::Element -&#62; new() above, it&#39;s basically the same:</p>

<dl>
<dt><a name="o_actions"
>o actions</a></dt>

<dd>
<p>This is the name of a Perl package in which actions such as do_add() and do_multiply() will be looked for. OK, so the lexer has no such option, the &#39;current&#39; package being the default.</p>

<dt><a name="o_start"
>o start</a></dt>

<dd>
<p>This is the <i>lhs</i> name of the rule to start with, as with the lexer.</p>

<dt><a name="o_rules"
>o rules</a></dt>

<dd>
<p>This is just an arrayref of <i>rule descriptors</i> defining the syntax of the grammar. This is the lexer&#39;s <i>transitions</i> parameter.</p>

<dt><a name="o_default_action"
>o default_action</a></dt>

<dd>
<p>Use this (optional) callback as the action for any rule element which does not explicitly specify its own action.</p>
</dd>
</dl>

<p>So our real problem is re-casting the syntax from BNF, or whatever, into a set of these (lhs, rhs, action) <i>rule descriptors</i>.</p>

<p>Now, how do we think about such a problem. I suggest contrast-and-compare real code with what the grammar says it must be:</p>

<p>Firstly, let&#39;s repeat <i>teamwork.dot</i> from above:</p>

<pre>        digraph Perl
        {
        graph [ rankdir=&#34;LR&#34; ]
        node  [ fontsize=&#34;12pt&#34; shape=&#34;rectangle&#34; style=&#34;filled, solid&#34; ]
        edge  [ color=&#34;grey&#34; ]
        &#34;Teamwork&#34; [ fillcolor=&#34;yellow&#34; ]
        &#34;Victory&#34;  [ fillcolor=&#34;red&#34; ]
        &#34;Teamwork&#34; -&#62; &#34;Victory&#34; [ label=&#34;is the key to&#34; ]
        }</pre>

<p>Generalizing, we know a <a href="http://www.graphviz.org/" class="podlinkurl"
>Graphviz</a> (DOT) graph must start like one of these:</p>

<pre>        strict digraph $id {...} # Case 1. $id is a variable.
        strict digraph     {...}
        strict   graph $id {...} # Case 3
        strict   graph     {...}
               digraph $id {...} # Case 5
               digraph     {...}
                 graph $id {...} # Case 7
                 graph     {...}</pre>

<p>As indeed the real code does, with the graph&#39;s id being <i>Perl</i> (i.e. case 5 in that list).</p>

<p>If you&#39;ve ever noticed that BNFs can be written as a tree (can they?), you&#39;ll know what comes next: We&#39;re going to start writing <i>rule descriptors</i> from the root down.</p>

<p>Drawing this as a tree gives:</p>

<pre>             DOT&#39;s Grammar
                  |
                  V
        ---------------------
        |                   |
     strict                 |
        |                   |
        ---------------------
                  |
                  V
        ---------------------
        |                   |
     digraph     or       graph
        |                   |
        ---------------------
                  |
                  V
        ---------------------
        |                   |
       $id                  |
        |                   |
        ---------------------
                  |
                  V
                {...}</pre>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Connecting_the_Parser_back_to_the_Lexer"
>Connecting the Parser back to the Lexer</a></h1>

<p>But wait, what&#39;s this? I&#39;ve just said in that tree that the <i>strict</i> is optional. Well, no it&#39;s not, in the parser. It is optional in the DOT language, but I designed the lexer, and I therein ensured it would necessarily output <i>strict =&#62; no</i> when the author of the graph omitted the <i>strict</i>.</p>

<p>So, by the time we&#39;re inside the parser, it&#39;s no longer optional, and that was done simply to make the life easier for consumers of the lexer&#39;s output stream, such as authors of parsers.</p>

<p>Likewise, for <i>digraph</i> &#39;v&#39; <i>graph</i>, I designed the lexer to output <i>digraph =&#62; &#39;yes&#39;</i> in one case and <i>digraph =&#62; &#39;no&#39;</i> in the other.</p>

<p>What does that mean? It means, for <i>teamwork.dot</i>, the lexer will output (in some convenient format) the equivalent of:</p>

<pre>        strict   =&#62; no
        digraph  =&#62; yes
        graph_id =&#62; Perl
        ...</pre>

<p><i>graph_id</i> was chosen because the DOT language allows other types of ids, e.g. for nodes, edges, ports and compass points.</p>

<p>This gives us our first 6 Marpa-friendly rules, embedded in an arrayref of rules:</p>

<pre>        [
        {   # Root-level stuff.
                lhs =&#62; &#39;graph_grammar&#39;,
                rhs =&#62; [qw/prolog_and_graph/],
        },
        {
                lhs =&#62; &#39;prolog_and_graph&#39;,
                rhs =&#62; [qw/prolog_definition graph_sequence_definition/],
        },
        {   # Prolog stuff.
                lhs =&#62; &#39;prolog_definition&#39;,
                rhs =&#62; [qw/strict_definition digraph_definition graph_id_definition/],
        },
        {
                lhs    =&#62; &#39;strict_definition&#39;,
                rhs    =&#62; [qw/strict/],
                action =&#62; &#39;strict&#39;, # &#60;== Callback.
        },
        {
                lhs    =&#62; &#39;digraph_definition&#39;,
                rhs    =&#62; [qw/digraph/],
                action =&#62; &#39;digraph&#39;, # &#60;== Callback.
        },
        {
                lhs    =&#62; &#39;graph_id_definition&#39;,
                rhs    =&#62; [qw/graph_id/],
                action =&#62; &#39;graph_id&#39;, # &#60;== Callback.
        },
        ...
        ]</pre>

<p>That is, we&#39;re saying the graph, as a whole, consists of:</p>

<dl>
<dt><a name="o_A_prolog_thingy"
>o A prolog thingy</a></dt>

<dd>
<p>And then...</p>

<dt><a name="o_A_graph_sequence_thingy"
>o A graph sequence thingy</a></dt>
</dl>

<p>Remember, those names &#39;prolog_and_graph&#39; etc are just ids I made up.</p>

<p>Next, a prolog consists of:</p>

<dl>
<dt><a name="o_A_strict_thingy"
>o A strict thingy</a></dt>

<dd>
<p>Which is now not optional, and then...</p>

<dt><a name="o_A_digraph_thingy"
>o A digraph thingy</a></dt>

<dd>
<p>Which will turn out to match the lexer input of /^(di|)graph$/, and the lexer output of digraph =&#62; /^(yes|no)$/, and then...</p>

<dt><a name="o_A_graph_id"
>o A graph_id</a></dt>

<dd>
<p>Which is optional, and then...</p>

<dt><a name="o_Some_other_stuff"
>o Some other stuff</a></dt>

<dd>
<p>Which will be the precise definition of real live graphs, represented above by {...} in the list of the 8 possible formats for the prolog.</p>
</dd>
</dl>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Something_Fascinating_about_Rule_Descriptors"
>Something Fascinating about Rule Descriptors</a></h1>

<p>But take another look at those rule descriptors. They say <i>nothing</i> about the values of the tokens! For instance, in <i>graph_id =&#62; Perl</i> what happens to ids such as <i>Perl</i>. Nothing. They are ignored. And that&#39;s because that&#39;s just how these grammars work.</p>

<p>Recall: It was the <i>lexer</i>&#39;s job to identify valid graph ids based on the 1st sub-grammar. By the time the data hits the parser, we know we have a valid graph id, and as long as it plugs in to the <i>structure</i> of the grammar in the right place, we are prepared to accept <i>any valid</i> graph id. Hence <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a> does not even look at the graph id, which is a way of saying this one grammar works with every valid graph id.</p>

<p>This point also raises the tricky discussion of whether a specific implementation of lexer/parser code can or must keep the 2 phases separate, or whether in fact you can roll them into one without falling for the &#39;premature optimisation&#39; trap. I&#39;ll just draw a veil over that discussion, since I&#39;ve already declared my stance: They&#39;re implemented in 2 modules.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Chains_and_Trees"
>Chains and Trees</a></h1>

<p>But if these rules have to be chained into a tree, how do we handle the root? Well, consider this call to <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a>&#39;s new() method:</p>

<pre>        my($grammar) = Marpa::R2::Grammar -&#62; new(... start =&#62; &#39;graph_grammar&#39;, ...);</pre>

<p>And <i>graph_grammar</i> is precisely the <i>lhs</i> in the first rule descriptor.</p>

<p>After that, every rule&#39;s <i>rhs</i>, including the root&#39;s, must be defined later in the list of rule descriptors. This forms the links in the chain, and if drawn you&#39;ll see the end result is a tree.</p>

<p>Here&#39;s the full <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a> grammar for DOT (as used in the <a href="http://metacpan.org/module/GraphViz2::Marpa" class="podlinkpod"
>GraphViz2::Marpa</a> module) as an image: <a href="http://savage.net.au/Ron/html/graphviz2.marpa/Marpa.Grammar.svg" class="podlinkurl"
>http://savage.net.au/Ron/html/graphviz2.marpa/Marpa.Grammar.svg</a>. This image was created with (you guessed it!) <a href="http://www.graphviz.org/" class="podlinkurl"
>Graphviz</a> via <a href="http://metacpan.org/module/GraphViz2" class="podlinkpod"
>GraphViz2</a>. Numbers have been added to node names in the tree, otherwise Graphviz would regard any 2 identical numberless names as one and the same node.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Less_Coding_-_More_Design"
>Less Coding - More Design</a></h1>

<p>Here I&#39;ll stop building the tree of the grammar (see the next article), and turn to some design issues.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="My_Rules-of-Thumb_for_Writing_Lexers/Parsers"
>My Rules-of-Thumb for Writing Lexers/Parsers</a></h1>

<p>The remainder of this document is to help beginners orient their thinking when confronted with a problem they don&#39;t have experience at tackling. Of course, if you&#39;re an expert in lexing and parsing, feel free to ignore everything I say.</p>

<p>And, if you think I&#39;ve misused lexing/parsing terminology here, please let me know.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Eschew_Premature_Optimisation"
>Eschew Premature Optimisation</a></h2>

<p>Yep, this old one again. It has various connotations:</p>

<dl>
<dt><a name="o_The_lexer_and_the_parser"
>o The lexer and the parser</a></dt>

<dd>
<p>Don&#39;t aim to combine the lexer and parser, even though that&#39;s what might eventuate.</p>

<p>I.e. wait until the design of each is clear and finalized, before trying to jam them into a single module (or program).</p>

<dt><a name="o_The_lexer_and_the_tokens"
>o The lexer and the tokens</a></dt>

<dd>
<p>Do make the lexer identify the existence of tokens, but not identify their ultimate role or meaning.</p>

<dt><a name="o_The_lexer_and_context"
>o The lexer and context</a></dt>

<dd>
<p>Don&#39;t make the lexer do context analysis.</p>

<p>Here I mean make the parser be the one to disambiguate tokens with multiple meanings, by using the context, which at this point are tokens identified by the lexer.</p>

<p>And <a href="http://en.wikipedia.org/wiki/Context_analysis" class="podlinkurl"
>context analysis for businesses</a>, for example, is probably not what you want either.</p>

<dt><a name="o_The_lexer_and_syntax"
>o The lexer and syntax</a></dt>

<dd>
<p>Don&#39;t make the lexer do syntax checking. This is effectively the same as the last point.</p>

<dt><a name="o_The_lexer_and_its_output"
>o The lexer and its output</a></dt>

<dd>
<p>Don&#39;t minimize the lexer&#39;s output stream. For instance, don&#39;t force the code which reads the lexer&#39;s output to guess whether or not a variable-length set of tokens has ended. Output a specific token as a set terminator. The point of this token is to tell the parser exactly what&#39;s going on. Without such a token, the next token has to do double-duty: Firstly it tells the parser the variable-length part has finished, and secondly, it represents itself. Such overloading is unnecessary.</p>

<dt><a name="o_The_State_Transition_Table"
>o The State Transition Table</a></dt>

<dd>
<p>In the STT, don&#39;t try to minimize the number of states, at least not until the code has stabilized (i.e. is no longer under [rapid] development).</p>

<p>I develop my STTs in a spreadsheet, which means a formula (regexp) stored in 1 cell can be referenced in any number of other cells. This is <i>very</i> convenient.</p>
</dd>
</dl>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Divide_and_Conquer"
>Divide and Conquer</a></h2>

<p>Hmmm, another ancient <a href="http://en.wikipedia.org/wiki/Aphorism" class="podlinkurl"
>aphorism</a>. Naturally, these persist precisely because they&#39;re telling us something important.</p>

<p>Here, it means study the problem carefully, and deal with each part (lexer, parser) of it separately. &#39;Nuff said.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Don&#39;t_Reinvent_the_Wheel"
>Don&#39;t Reinvent the Wheel</a></h2>

<p>Yes, I know <i>you&#39;d</i> never do that.</p>

<p>Anyway, there are Perl modules to help with things like the STT. E.g.: <a href="http://metacpan.org/module/Set::FA::Element" class="podlinkpod"
>Set::FA::Element</a>. Check its &#39;See Also&#39; (in <a href="http://metacpan.org/module/Set::FA" class="podlinkpod"
>Set::FA</a>, actually) for other STT helpers.</p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Be_Patient_with_the_STT"
>Be Patient with the STT</a></h2>

<p>Developing the STT takes many iterations:</p>

<dl>
<dt><a name="o_The_test_cases"
>o The test cases</a></dt>

<dd>
<p>For each iteration, prepare a separate test case.</p>

<dt><a name="o_The_tiny_script"
>o The tiny script</a></dt>

<dd>
<p>Have a tiny script which runs 1 test. Giving it a short, perhaps temporary, name, makes each test just that little bit easier to run.</p>

<p>By temporary name I mean you can give it a meaningful name later, when including it in the distro.</p>

<dt><a name="o_The_wrapper_script"
>o The wrapper script</a></dt>

<dd>
<p>Have a script which runs all tests.</p>

<p>I keep the test data files in the data/ dir, and the scripts in the scripts/ dir. Then, creating tests in the t/ dir can perhaps utilize these two sets of helpers.</p>

<p>Since I&#39;ve only used <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a> for graphical work, the output of the wrapper is a web page, which makes viewing the results simple. I like to include (short) input or output text files on such a page, beside the *.svg images. That way I can see at a glance what the input was and hence I can tell what the output should be without switching to the editor&#39;s window.</p>

<p>There&#39;s a little bit of effort initially, but after that it&#39;s <i>so</i> easy to check the output of the latest test.</p>
</dd>
</dl>

<p>Sample output from my wrapper scripts:</p>

<p><a href="http://savage.net.au/Perl-modules/html/graphviz2/" class="podlinkurl"
>GraphViz2 (non-Marpa)</a></p>

<p><a href="http://savage.net.au/Perl-modules/html/graphviz2.marpa/" class="podlinkurl"
>GraphViz2::Marpa</a></p>

<p><a href="http://savage.net.au/Perl-modules/html/graphviz2.pathutils/" class="podlinkurl"
>GraphViz2::Marpa::PathUtils</a></p>

<p><a href="http://savage.net.au/Perl-modules/html/graph.easy.marpa/" class="podlinkurl"
>Graph::Easy::Marpa</a></p>

<h2><a class='u' href='#___top' title='click to go to top of document'
name="Be_Patient_with_the_Grammar"
>Be Patient with the Grammar</a></h2>

<p>As with the STT, this, at least for me, is very much a trial-and-error process.</p>

<p>Tips:</p>

<dl>
<dt><a name="o_Paper,_not_code"
>o Paper, not code</a></dt>

<dd>
<p>A good idea is not to start by coding with your editor, but to draw the grammar as a tree, on paper.</p>

<dt><a name="o_Watch_out_for_alternatives"
>o Watch out for alternatives</a></dt>

<dd>
<p>This refers to when one of several tokens can appear in the input stream. Learn exactly how to draw that without trying to minimize (see above) the number of branches in the tree.</p>

<p>Of course, you will still need to learn how to code such a construct. Here&#39;s a bit of code from <a href="http://metacpan.org/module/Graph::Easy::Marpa" class="podlinkpod"
>Graph::Easy::Marpa</a> which deals with this (note: we&#39;re back to the Graph::Easy language from here on!):</p>

<pre>        {   # Graph stuff.
                lhs =&#62; &#39;graph_definition&#39;,
                rhs =&#62; [qw/graph_statement/],
        },
        {
                lhs =&#62; &#39;graph_statement&#39;, # 1 of 3.
                rhs =&#62; [qw/group_definition/],
        },
        {
                lhs =&#62; &#39;graph_statement&#39;, # 2 of 3.
                rhs =&#62; [qw/node_definition/],
        },
        {
                lhs =&#62; &#39;graph_statement&#39;, # 3 of 3.
                rhs =&#62; [qw/edge_definition/],
        },</pre>

<p>This is telling you that a graph thingy can be any one of a group, node or edge. It&#39;s <a href="http://metacpan.org/module/Marpa::R2" class="podlinkpod"
>Marpa::R2</a>&#39;s job to try the 1/2/3 of 3 in order, to see which (if any) matches the input stream.</p>

<p>So, this represents a point in the input stream where one of several <i>alternatives</i> can appear.</p>

<p>The tree would look like:</p>

<pre>                        graph_definition
                               |
                               V
                        graph_statement
                               |
                               V
            ---------------------------------------
            |                  |                  |
            V                  V                  V
     group_definition   node_definition    edge_definition</pre>

<p>The comment &#39;3 of 3&#39;, for instance, says an edge can stand alone.</p>

<dt><a name="o_Watch_out_for_sequences"
>o Watch out for sequences</a></dt>

<dd>
<p>But consider the <i>node_definition</i>:</p>

<pre>        {   # Node stuff.
                lhs =&#62; &#39;node_definition&#39;,
                rhs =&#62; [qw/node_sequence/],
                min =&#62; 0,
        },
        {
                lhs =&#62; &#39;node_sequence&#39;, # 1 of 4.
                rhs =&#62; [qw/node_statement/],
        },
        {
                lhs =&#62; &#39;node_sequence&#39;, # 2 of 4.
                rhs =&#62; [qw/node_statement daisy_chain_node/],
        },
        {
                lhs =&#62; &#39;node_sequence&#39;, # 3 of 4.
                rhs =&#62; [qw/node_statement edge_definition/],
        },
        {
                lhs =&#62; &#39;node_sequence&#39;, # 4 of 4.
                rhs =&#62; [qw/node_statement group_definition/],
        },</pre>

<p>Here the comment &#39;3 of 4&#39; tells you that nodes can be followed by edges.</p>

<p>A realistic sample is: [node_1] -&#62; [node_2], where &#39;[x]&#39; is a node and &#39;-&#62;&#39; is an edge, because an edge can be followed by a node (applying &#39;3 of 4&#39; below).</p>

<p>So, this (above and below) represents a point in the input stream where one of several specific <i>sequences</i> of tokens are allowed/expected. Here&#39;s the <i>edge_definition</i>:</p>

<pre>        {   # Edge stuff.
                lhs =&#62; &#39;edge_definition&#39;,
                rhs =&#62; [qw/edge_sequence/],
                min =&#62; 0,
        },
        {
                lhs =&#62; &#39;edge_sequence&#39;, # 1 of 4.
                rhs =&#62; [qw/edge_statement/],
        },
        {
                lhs =&#62; &#39;edge_sequence&#39;, # 2 of 4.
                rhs =&#62; [qw/edge_statement daisy_chain_edge/],
        },
        {
                lhs =&#62; &#39;edge_sequence&#39;, # 3 of 4.
                rhs =&#62; [qw/edge_statement node_definition/],
        },
        {
                lhs =&#62; &#39;edge_sequence&#39;, # 4 of 4.
                rhs =&#62; [qw/edge_statement group_definition/],
        },
        {
                lhs =&#62; &#39;edge_statement&#39;,
                rhs =&#62; [qw/edge_name attribute_definition/],
        },
        {
                lhs    =&#62; &#39;edge_name&#39;,
                rhs    =&#62; [qw/edge_id/],
                action =&#62; &#39;edge_id&#39;,
        },</pre>
</dd>
</dl>

<p>But, I have to stop somewhere, so...</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Wrapping_Up_and_Winding_Down"
>Wrapping Up and Winding Down</a></h1>

<p>I hope I&#39;ve clarified what can be a complex and daunting part of programming, and I also hope I&#39;ve convinced you that working in Perl, with the help of a spreadsheet, is the modern aka only way to lexer and parser bliss.</p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Author"
>Author</a></h1>

<p>Ron Savage <img src="http://savage.net.au/assets/images/local/email-address.png" />.</p>

<p>Home page: <a href="http://savage.net.au/index.html" class="podlinkurl"
>http://savage.net.au/index.html</a></p>

<h1><a class='u' href='#___top' title='click to go to top of document'
name="Licence"
>Licence</a></h1>

<p>Australian Copyright &copy; 2012 Ron Savage. All rights reserved.</p>

<pre>        All Programs of mine are &#39;OSI Certified Open Source Software&#39;;
        you can redistribute them and/or modify them under the terms of
        The Artistic License, a copy of which is available at:
        http://www.opensource.org/licenses/index.html</pre>

<!-- end doc -->

</body></html>
